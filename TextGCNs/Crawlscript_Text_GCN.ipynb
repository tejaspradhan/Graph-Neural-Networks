{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawlscript Text GCN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaspradhan/Graph-Neural-Networks/blob/main/TextGCNs/Crawlscript_Text_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9EktHzxEzONR",
        "outputId": "afaaba70-133b-4aa6-b828-e7841e0228a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "Rz_7cPkA1DqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U tf_geometric"
      ],
      "metadata": {
        "id": "J-UC6Sdg0OgW",
        "outputId": "b636a319-a74d-468c-e8a4-14ca043521f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf_geometric\n",
            "  Downloading tf_geometric-0.0.81.tar.gz (57 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 30 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 57 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting tf_sparse>=0.0.10\n",
            "  Downloading tf_sparse-0.0.10.tar.gz (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.17.4 in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (1.21.5)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (2.6.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (1.0.2)\n",
            "Collecting ogb_lite>=0.0.3\n",
            "  Downloading ogb_lite-0.0.3.tar.gz (25 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (4.63.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb_lite>=0.0.3->tf_geometric) (1.3.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb_lite>=0.0.3->tf_geometric) (1.15.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb_lite>=0.0.3->tf_geometric) (1.24.3)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb_lite>=0.0.3->tf_geometric) (2.23.0)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb_lite>=0.0.3->tf_geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb_lite>=0.0.3->tf_geometric) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->tf_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->tf_geometric) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb_lite>=0.0.3->tf_geometric) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb_lite>=0.0.3->tf_geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb_lite>=0.0.3->tf_geometric) (2.10)\n",
            "Building wheels for collected packages: tf-geometric, ogb-lite, tf-sparse, littleutils\n",
            "  Building wheel for tf-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf-geometric: filename=tf_geometric-0.0.81-py3-none-any.whl size=83616 sha256=addfcdcd187ef2dad1b14cb86520842ce5b203ec7d5c09a68d6b56d96fcf30b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/96/bd/40006d7b40d830014b65c59b8494145713b9ac9236f04e678e\n",
            "  Building wheel for ogb-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ogb-lite: filename=ogb_lite-0.0.3-py3-none-any.whl size=33524 sha256=dc3130f4d5978d0878903316ed3f9bc9a9e5e2df59594e9d2b196e433bc4eab0\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/98/b1/5d034ba69940a93aa9b8e05f6e2f630f47b045209bc3cfa210\n",
            "  Building wheel for tf-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf-sparse: filename=tf_sparse-0.0.10-py3-none-any.whl size=8412 sha256=d23b57b1baf44a2c092348195005efb0358d7f80fd31087d391d0dcfd1c02430\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/a8/f3/d2bbee94f874cd00b6be8e70da2493e84e9a4775944d913728\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=28a8af0ba8fb5bf36bd04c159d7fffdc5b085cdc728f01fb8415f8beda1b4a76\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built tf-geometric ogb-lite tf-sparse littleutils\n",
            "Installing collected packages: littleutils, outdated, tf-sparse, ogb-lite, tf-geometric\n",
            "Successfully installed littleutils-0.2.2 ogb-lite-0.0.3 outdated-0.2.1 tf-geometric-0.0.81 tf-sparse-0.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f2k_1-d2w90l",
        "outputId": "3d52b855-e9b5-4138-b487-5b50b210198f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/fft/__init__.py:97: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
            "  from numpy.dual import register_func\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/sputils.py:17: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
            "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio5.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from .mio5_utils import VarReader5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tf_geometric.utils import tf_utils\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tf_geometric as tfg\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "EJ2v1DBW080q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/\"\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "for fname, label in [[\"rt-polarity.pos\", 1], [\"rt-polarity.neg\", 0]]:\n",
        "    fpath = os.path.join(data_dir, fname)\n",
        "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            texts.append(line.strip())\n",
        "            labels.append(label)\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)"
      ],
      "metadata": {
        "id": "RidKM2uFxeJl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:5]"
      ],
      "metadata": {
        "id": "0hCH7NTus9UC",
        "outputId": "7a055790-b0f9-4b6a-fafb-280188f9b134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
              " 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n",
              " 'effective but too-tepid biopic',\n",
              " 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .',\n",
              " \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training samples : {len(train_sequences)}, Test Samples : {len(test_sequences)} ')"
      ],
      "metadata": {
        "id": "GOj_aC5VxxtQ",
        "outputId": "e7c28f17-ff8a-4056-e992-13f8246c7012",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples : 8529, Test Samples : 2133 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the PMI Model for calculating pointwise mutual info for preprocessing as a graph"
      ],
      "metadata": {
        "id": "UcQAGYc51l2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PMIModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_counter = None\n",
        "        self.pair_counter = None\n",
        "\n",
        "    def get_pair_id(self, word0, word1):\n",
        "        pair_id = tuple(sorted([word0, word1]))\n",
        "        return pair_id\n",
        "\n",
        "    def fit(self, sequences, window_size):\n",
        "\n",
        "        self.word_counter = Counter()\n",
        "        self.pair_counter = Counter()\n",
        "        num_windows = 0\n",
        "        for sequence in tqdm(sequences):\n",
        "            for offset in range(len(sequence) - window_size):\n",
        "                window = sequence[offset:offset + window_size]\n",
        "                num_windows += 1\n",
        "                for i, word0 in enumerate(window):\n",
        "                    self.word_counter[word0] += 1\n",
        "                    for j, word1 in enumerate(window[i + 1:]):\n",
        "                        pair_id = self.get_pair_id(word0, word1)\n",
        "                        self.pair_counter[pair_id] += 1\n",
        "\n",
        "        for word, count in self.word_counter.items():\n",
        "            self.word_counter[word] = count / num_windows\n",
        "        for pair_id, count in self.pair_counter.items():\n",
        "            self.pair_counter[pair_id] = count / num_windows\n",
        "\n",
        "    def transform(self, word0, word1):\n",
        "        prob_a = self.word_counter[word0]\n",
        "        prob_b = self.word_counter[word1]\n",
        "        pair_id = self.get_pair_id(word0, word1)\n",
        "        prob_pair = self.pair_counter[pair_id]\n",
        "\n",
        "        if prob_a == 0 or prob_b == 0 or prob_pair == 0:\n",
        "            return 0\n",
        "\n",
        "        pmi = np.log(prob_pair / (prob_a * prob_b))\n",
        "        # print(word0, word1, pmi)\n",
        "        pmi = np.maximum(pmi, 0.0)\n",
        "        # print(pmi)\n",
        "        return pmi"
      ],
      "metadata": {
        "id": "oorDFMHt1hoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to Create Graphs\n",
        "* Each word in the sentence represents a node of the graph.\n",
        "* If two words occur together they're connected with an edge.\n",
        "* The weight of the edge will be equal to the PMI score of the two words.\n"
      ],
      "metadata": {
        "id": "Zr6PyJdI2Fig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_word_graph(num_words, pmi_model, embedding_size):\n",
        "    x = tf.Variable(tf.random.truncated_normal([num_words, embedding_size], stddev=1 / np.sqrt(embedding_size)),\n",
        "                    dtype=tf.float32)\n",
        "    edges = []\n",
        "    edge_weight = []\n",
        "    for (word0, word1) in pmi_model.pair_counter.keys():\n",
        "        pmi = pmi_model.transform(word0, word1)\n",
        "        if pmi > 0:\n",
        "            edges.append([word0, word1])\n",
        "            edge_weight.append(pmi)\n",
        "            edges.append([word1, word0])\n",
        "            edge_weight.append(pmi)\n",
        "    edge_index = np.array(edges).T\n",
        "    return tfg.Graph(x=x, edge_index=edge_index, edge_weight=edge_weight)\n"
      ],
      "metadata": {
        "id": "T6OkZt-W1rMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_combined_graph(word_graph, sequences, embedding_size):\n",
        "    num_words = word_graph.num_nodes\n",
        "    x = tf.zeros([len(sequences), embedding_size], dtype=tf.float32)\n",
        "    edges = []\n",
        "    edge_weight = []\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        doc_node_index = num_words + i\n",
        "        for word in sequence:\n",
        "            edges.append([doc_node_index, word])  # only directed edge\n",
        "            edge_weight.append(1.0)  # use BOW instaead of TF-IDF\n",
        "\n",
        "    edge_index = np.array(edges).T\n",
        "    x = tf.concat([word_graph.x, x], axis=0)\n",
        "    edge_index = np.concatenate([word_graph.edge_index, edge_index], axis=1)\n",
        "    edge_weight = np.concatenate([word_graph.edge_weight, edge_weight], axis=0)\n",
        "    return tfg.Graph(x=x, edge_index=edge_index, edge_weight=edge_weight)"
      ],
      "metadata": {
        "id": "DSCT9ykD2mJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmi_cache_path = \"cached_pmi_model.p\"\n",
        "if os.path.exists(pmi_cache_path):\n",
        "    with open(pmi_cache_path, \"rb\") as f:\n",
        "        pmi_model = pickle.load(f)\n",
        "else:\n",
        "    pmi_model = PMIModel()\n",
        "    pmi_model.fit(train_sequences, window_size=6)\n",
        "    with open(pmi_cache_path, \"wb\") as f:\n",
        "        pickle.dump(pmi_model, f)\n",
        "\n",
        "embedding_size = 150\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "word_graph = build_word_graph(num_words, pmi_model, embedding_size)\n",
        "train_combined_graph = build_combined_graph(word_graph, train_sequences, embedding_size)\n",
        "test_combined_graph = build_combined_graph(word_graph, test_sequences, embedding_size)\n",
        "\n",
        "print(word_graph)\n",
        "print(train_combined_graph)\n",
        "print(test_combined_graph)\n",
        "\n",
        "num_classes = 2"
      ],
      "metadata": {
        "id": "TSWMNPSt2mbs",
        "outputId": "ee2e272e-e19a-4a2e-e7ac-1f77d3bf8c14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Shape: x => (17515, 150)\tedge_index => (2, 570980)\ty => None\n",
            "Graph Shape: x => (26044, 150)\tedge_index => (2, 732610)\ty => None\n",
            "Graph Shape: x => (19648, 150)\tedge_index => (2, 608633)\ty => None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.gcn0 = tfg.layers.GCN(100, activation=tf.nn.relu)\n",
        "        self.gcn1 = tfg.layers.GCN(num_classes)\n",
        "        self.dropout = keras.layers.Dropout(0.5)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None, cache=None):\n",
        "        x, edge_index, edge_weight = inputs\n",
        "        h = self.gcn0([x, edge_index, edge_weight], cache=cache)\n",
        "        h = self.dropout(h, training=training)\n",
        "        h = self.gcn1([h, edge_index, edge_weight], cache=cache)\n",
        "        return h"
      ],
      "metadata": {
        "id": "u6zVjRNd2r2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCNModel()\n",
        "model.gcn0.cache_normed_edge(train_combined_graph)\n",
        "model.gcn0.cache_normed_edge(test_combined_graph)"
      ],
      "metadata": {
        "id": "aojMwmhn3NE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf_utils.function\n",
        "def forward(graph, training=False):\n",
        "    logits = model([graph.x, graph.edge_index, graph.edge_weight], cache=graph.cache, training=training)\n",
        "    logits = logits[num_words:]\n",
        "    return logits"
      ],
      "metadata": {
        "id": "ko12FT-v3PcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(logits, labels):\n",
        "    losses = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        logits=logits,\n",
        "        labels=tf.one_hot(labels, depth=num_classes)\n",
        "    )\n",
        "    mean_loss = tf.reduce_mean(losses)\n",
        "    return mean_loss"
      ],
      "metadata": {
        "id": "auhu2FtG3SZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-2)"
      ],
      "metadata": {
        "id": "3koCRTwC3fxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(1000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = forward(train_combined_graph, training=True)\n",
        "        mean_loss = compute_loss(logits, train_labels)\n",
        "\n",
        "    vars = tape.watched_variables()\n",
        "    grads = tape.gradient(mean_loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        logits = forward(test_combined_graph)\n",
        "        preds = tf.argmax(logits, axis=-1)\n",
        "        corrects = tf.cast(tf.equal(preds, test_labels), tf.float32)\n",
        "        accuracy = tf.reduce_mean(corrects)\n",
        "        print(\"step = {}\\tloss = {}\\ttest_accuracy = {}\".format(step, mean_loss, accuracy))"
      ],
      "metadata": {
        "id": "CtbvrpJP3UIi",
        "outputId": "93092e99-e1cb-4b8e-c7a3-b396a1f4bf17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 420\tloss = 0.40709564089775085\ttest_accuracy = 0.6957337260246277\n",
            "step = 430\tloss = 0.42121386528015137\ttest_accuracy = 0.6938584446907043\n",
            "step = 440\tloss = 0.4310871362686157\ttest_accuracy = 0.6971401572227478\n",
            "step = 450\tloss = 0.41143521666526794\ttest_accuracy = 0.688701331615448\n",
            "step = 460\tloss = 0.42778322100639343\ttest_accuracy = 0.6821378469467163\n",
            "step = 470\tloss = 0.38684967160224915\ttest_accuracy = 0.6849507689476013\n",
            "step = 480\tloss = 0.40419355034828186\ttest_accuracy = 0.6952648758888245\n",
            "step = 490\tloss = 0.4028474986553192\ttest_accuracy = 0.7018284201622009\n",
            "step = 500\tloss = 0.4259778559207916\ttest_accuracy = 0.6821378469467163\n",
            "step = 510\tloss = 0.4004305303096771\ttest_accuracy = 0.6769807934761047\n",
            "step = 520\tloss = 0.4100584089756012\ttest_accuracy = 0.6952648758888245\n",
            "step = 530\tloss = 0.3987284302711487\ttest_accuracy = 0.703234851360321\n",
            "step = 540\tloss = 0.41889843344688416\ttest_accuracy = 0.7051101922988892\n",
            "step = 550\tloss = 0.4155591130256653\ttest_accuracy = 0.7022972106933594\n",
            "step = 560\tloss = 0.3977575898170471\ttest_accuracy = 0.7046413421630859\n",
            "step = 570\tloss = 0.4006323218345642\ttest_accuracy = 0.7065166234970093\n",
            "step = 580\tloss = 0.39414018392562866\ttest_accuracy = 0.6980777978897095\n",
            "step = 590\tloss = 0.3982029855251312\ttest_accuracy = 0.6971401572227478\n",
            "step = 600\tloss = 0.40112754702568054\ttest_accuracy = 0.700421929359436\n",
            "step = 610\tloss = 0.40060955286026\ttest_accuracy = 0.6840131282806396\n",
            "step = 620\tloss = 0.3955761790275574\ttest_accuracy = 0.6901078224182129\n",
            "step = 630\tloss = 0.39344701170921326\ttest_accuracy = 0.7018284201622009\n",
            "step = 640\tloss = 0.38515934348106384\ttest_accuracy = 0.7018284201622009\n",
            "step = 650\tloss = 0.3916786313056946\ttest_accuracy = 0.7008907794952393\n",
            "step = 660\tloss = 0.3846017122268677\ttest_accuracy = 0.6793248653411865\n",
            "step = 670\tloss = 0.39800846576690674\ttest_accuracy = 0.7102672457695007\n",
            "step = 680\tloss = 0.3977467119693756\ttest_accuracy = 0.7093296051025391\n",
            "step = 690\tloss = 0.3824683725833893\ttest_accuracy = 0.707454264163971\n",
            "step = 700\tloss = 0.38839593529701233\ttest_accuracy = 0.7079231142997742\n",
            "step = 710\tloss = 0.3916591703891754\ttest_accuracy = 0.6985466480255127\n",
            "step = 720\tloss = 0.3876110315322876\ttest_accuracy = 0.694796085357666\n",
            "step = 730\tloss = 0.37524738907814026\ttest_accuracy = 0.7037037014961243\n",
            "step = 740\tloss = 0.3918873965740204\ttest_accuracy = 0.7018284201622009\n",
            "step = 750\tloss = 0.3899511694908142\ttest_accuracy = 0.7041725516319275\n",
            "step = 760\tloss = 0.3805849850177765\ttest_accuracy = 0.6840131282806396\n",
            "step = 770\tloss = 0.373352974653244\ttest_accuracy = 0.6990154981613159\n",
            "step = 780\tloss = 0.38463160395622253\ttest_accuracy = 0.7051101922988892\n",
            "step = 790\tloss = 0.38647404313087463\ttest_accuracy = 0.7051101922988892\n",
            "step = 800\tloss = 0.39943256974220276\ttest_accuracy = 0.7051101922988892\n",
            "step = 810\tloss = 0.3805329501628876\ttest_accuracy = 0.7022972106933594\n",
            "step = 820\tloss = 0.39958277344703674\ttest_accuracy = 0.7008907794952393\n",
            "step = 830\tloss = 0.3877773582935333\ttest_accuracy = 0.6994842886924744\n",
            "step = 840\tloss = 0.3573869466781616\ttest_accuracy = 0.7055789828300476\n",
            "step = 850\tloss = 0.3787737786769867\ttest_accuracy = 0.703234851360321\n",
            "step = 860\tloss = 0.37821993231773376\ttest_accuracy = 0.694796085357666\n",
            "step = 870\tloss = 0.3829367458820343\ttest_accuracy = 0.7041725516319275\n",
            "step = 880\tloss = 0.3823745548725128\ttest_accuracy = 0.7088607549667358\n",
            "step = 890\tloss = 0.3814752399921417\ttest_accuracy = 0.6985466480255127\n",
            "step = 900\tloss = 0.38243401050567627\ttest_accuracy = 0.6966713666915894\n",
            "step = 910\tloss = 0.3774719834327698\ttest_accuracy = 0.7060478329658508\n",
            "step = 920\tloss = 0.3845384120941162\ttest_accuracy = 0.7065166234970093\n",
            "step = 930\tloss = 0.37696683406829834\ttest_accuracy = 0.7027660608291626\n",
            "step = 940\tloss = 0.38222184777259827\ttest_accuracy = 0.7027660608291626\n",
            "step = 950\tloss = 0.3724784851074219\ttest_accuracy = 0.6957337260246277\n",
            "step = 960\tloss = 0.36793604493141174\ttest_accuracy = 0.7041725516319275\n",
            "step = 970\tloss = 0.368844598531723\ttest_accuracy = 0.6985466480255127\n",
            "step = 980\tloss = 0.35775282979011536\ttest_accuracy = 0.7060478329658508\n",
            "step = 990\tloss = 0.37320730090141296\ttest_accuracy = 0.7093296051025391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-iwPbQl13WHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}