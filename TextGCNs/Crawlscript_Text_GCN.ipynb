{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawlscript Text GCN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaspradhan/Graph-Neural-Networks/blob/main/TextGCNs/Crawlscript_Text_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9EktHzxEzONR",
        "outputId": "d215cf9b-4e39-4471-b367-6e10e61a69f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "Rz_7cPkA1DqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U tf_geometric"
      ],
      "metadata": {
        "id": "J-UC6Sdg0OgW",
        "outputId": "a7c2b6b1-106f-4d0f-b51c-e8ba0f7a7f03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: littleutils, outdated, tf-sparse, ogb-lite, tf-geometric\n",
            "Successfully installed littleutils-0.2.2 ogb-lite-0.0.3 outdated-0.2.1 tf-geometric-0.0.81 tf-sparse-0.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2k_1-d2w90l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tf_geometric.utils import tf_utils\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tf_geometric as tfg\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "EJ2v1DBW080q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/\"\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "for fname, label in [[\"rt-polarity.pos\", 1], [\"rt-polarity.neg\", 0]]:\n",
        "    fpath = os.path.join(data_dir, fname)\n",
        "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            texts.append(line.strip())\n",
        "            labels.append(label)\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)"
      ],
      "metadata": {
        "id": "RidKM2uFxeJl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Training samples : {len(train_sequences)}, Test Samples : {len(test_sequences)} ')"
      ],
      "metadata": {
        "id": "GOj_aC5VxxtQ",
        "outputId": "e7c28f17-ff8a-4056-e992-13f8246c7012",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples : 8529, Test Samples : 2133 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the PMI Model for calculating pointwise mutual info for preprocessing as a graph"
      ],
      "metadata": {
        "id": "UcQAGYc51l2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PMIModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_counter = None\n",
        "        self.pair_counter = None\n",
        "\n",
        "    def get_pair_id(self, word0, word1):\n",
        "        pair_id = tuple(sorted([word0, word1]))\n",
        "        return pair_id\n",
        "\n",
        "    def fit(self, sequences, window_size):\n",
        "\n",
        "        self.word_counter = Counter()\n",
        "        self.pair_counter = Counter()\n",
        "        num_windows = 0\n",
        "        for sequence in tqdm(sequences):\n",
        "            for offset in range(len(sequence) - window_size):\n",
        "                window = sequence[offset:offset + window_size]\n",
        "                num_windows += 1\n",
        "                for i, word0 in enumerate(window):\n",
        "                    self.word_counter[word0] += 1\n",
        "                    for j, word1 in enumerate(window[i + 1:]):\n",
        "                        pair_id = self.get_pair_id(word0, word1)\n",
        "                        self.pair_counter[pair_id] += 1\n",
        "\n",
        "        for word, count in self.word_counter.items():\n",
        "            self.word_counter[word] = count / num_windows\n",
        "        for pair_id, count in self.pair_counter.items():\n",
        "            self.pair_counter[pair_id] = count / num_windows\n",
        "\n",
        "    def transform(self, word0, word1):\n",
        "        prob_a = self.word_counter[word0]\n",
        "        prob_b = self.word_counter[word1]\n",
        "        pair_id = self.get_pair_id(word0, word1)\n",
        "        prob_pair = self.pair_counter[pair_id]\n",
        "\n",
        "        if prob_a == 0 or prob_b == 0 or prob_pair == 0:\n",
        "            return 0\n",
        "\n",
        "        pmi = np.log(prob_pair / (prob_a * prob_b))\n",
        "        # print(word0, word1, pmi)\n",
        "        pmi = np.maximum(pmi, 0.0)\n",
        "        # print(pmi)\n",
        "        return pmi"
      ],
      "metadata": {
        "id": "oorDFMHt1hoy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to Create Graphs\n",
        "* Each word in the sentence represents a node of the graph.\n",
        "* If two words occur together they're connected with an edge.\n",
        "* The weight of the edge will be equal to the PMI score of the two words.\n"
      ],
      "metadata": {
        "id": "Zr6PyJdI2Fig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_word_graph(num_words, pmi_model, embedding_size):\n",
        "    x = tf.Variable(tf.random.truncated_normal([num_words, embedding_size], stddev=1 / np.sqrt(embedding_size)),\n",
        "                    dtype=tf.float32)\n",
        "    edges = []\n",
        "    edge_weight = []\n",
        "    for (word0, word1) in pmi_model.pair_counter.keys():\n",
        "        pmi = pmi_model.transform(word0, word1)\n",
        "        if pmi > 0:\n",
        "            edges.append([word0, word1])\n",
        "            edge_weight.append(pmi)\n",
        "            edges.append([word1, word0])\n",
        "            edge_weight.append(pmi)\n",
        "    edge_index = np.array(edges).T\n",
        "    return tfg.Graph(x=x, edge_index=edge_index, edge_weight=edge_weight)\n"
      ],
      "metadata": {
        "id": "T6OkZt-W1rMb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_combined_graph(word_graph, sequences, embedding_size):\n",
        "    num_words = word_graph.num_nodes\n",
        "    x = tf.zeros([len(sequences), embedding_size], dtype=tf.float32)\n",
        "    edges = []\n",
        "    edge_weight = []\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        doc_node_index = num_words + i\n",
        "        for word in sequence:\n",
        "            edges.append([doc_node_index, word])  # only directed edge\n",
        "            edge_weight.append(1.0)  # use BOW instaead of TF-IDF\n",
        "\n",
        "    edge_index = np.array(edges).T\n",
        "    x = tf.concat([word_graph.x, x], axis=0)\n",
        "    edge_index = np.concatenate([word_graph.edge_index, edge_index], axis=1)\n",
        "    edge_weight = np.concatenate([word_graph.edge_weight, edge_weight], axis=0)\n",
        "    return tfg.Graph(x=x, edge_index=edge_index, edge_weight=edge_weight)"
      ],
      "metadata": {
        "id": "DSCT9ykD2mJp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmi_cache_path = \"cached_pmi_model.p\"\n",
        "if os.path.exists(pmi_cache_path):\n",
        "    with open(pmi_cache_path, \"rb\") as f:\n",
        "        pmi_model = pickle.load(f)\n",
        "else:\n",
        "    pmi_model = PMIModel()\n",
        "    pmi_model.fit(train_sequences, window_size=6)\n",
        "    with open(pmi_cache_path, \"wb\") as f:\n",
        "        pickle.dump(pmi_model, f)\n",
        "\n",
        "embedding_size = 150\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "word_graph = build_word_graph(num_words, pmi_model, embedding_size)\n",
        "train_combined_graph = build_combined_graph(word_graph, train_sequences, embedding_size)\n",
        "test_combined_graph = build_combined_graph(word_graph, test_sequences, embedding_size)\n",
        "\n",
        "print(word_graph)\n",
        "print(train_combined_graph)\n",
        "print(test_combined_graph)\n",
        "\n",
        "num_classes = 2"
      ],
      "metadata": {
        "id": "TSWMNPSt2mbs",
        "outputId": "ee2e272e-e19a-4a2e-e7ac-1f77d3bf8c14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Shape: x => (17515, 150)\tedge_index => (2, 570980)\ty => None\n",
            "Graph Shape: x => (26044, 150)\tedge_index => (2, 732610)\ty => None\n",
            "Graph Shape: x => (19648, 150)\tedge_index => (2, 608633)\ty => None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.gcn0 = tfg.layers.GCN(100, activation=tf.nn.relu)\n",
        "        self.gcn1 = tfg.layers.GCN(num_classes)\n",
        "        self.dropout = keras.layers.Dropout(0.5)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None, cache=None):\n",
        "        x, edge_index, edge_weight = inputs\n",
        "        h = self.gcn0([x, edge_index, edge_weight], cache=cache)\n",
        "        h = self.dropout(h, training=training)\n",
        "        h = self.gcn1([h, edge_index, edge_weight], cache=cache)\n",
        "        return h"
      ],
      "metadata": {
        "id": "u6zVjRNd2r2M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCNModel()\n",
        "model.gcn0.cache_normed_edge(train_combined_graph)\n",
        "model.gcn0.cache_normed_edge(test_combined_graph)"
      ],
      "metadata": {
        "id": "aojMwmhn3NE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf_utils.function\n",
        "def forward(graph, training=False):\n",
        "    logits = model([graph.x, graph.edge_index, graph.edge_weight], cache=graph.cache, training=training)\n",
        "    logits = logits[num_words:]\n",
        "    return logits"
      ],
      "metadata": {
        "id": "ko12FT-v3PcN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(logits, labels):\n",
        "    losses = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        logits=logits,\n",
        "        labels=tf.one_hot(labels, depth=num_classes)\n",
        "    )\n",
        "    mean_loss = tf.reduce_mean(losses)\n",
        "    return mean_loss"
      ],
      "metadata": {
        "id": "auhu2FtG3SZZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-2)"
      ],
      "metadata": {
        "id": "3koCRTwC3fxy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(1000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = forward(train_combined_graph, training=True)\n",
        "        mean_loss = compute_loss(logits, train_labels)\n",
        "\n",
        "    vars = tape.watched_variables()\n",
        "    grads = tape.gradient(mean_loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        logits = forward(test_combined_graph)\n",
        "        preds = tf.argmax(logits, axis=-1)\n",
        "        corrects = tf.cast(tf.equal(preds, test_labels), tf.float32)\n",
        "        accuracy = tf.reduce_mean(corrects)\n",
        "        print(\"step = {}\\tloss = {}\\ttest_accuracy = {}\".format(step, mean_loss, accuracy))"
      ],
      "metadata": {
        "id": "CtbvrpJP3UIi",
        "outputId": "93092e99-e1cb-4b8e-c7a3-b396a1f4bf17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 420\tloss = 0.40709564089775085\ttest_accuracy = 0.6957337260246277\n",
            "step = 430\tloss = 0.42121386528015137\ttest_accuracy = 0.6938584446907043\n",
            "step = 440\tloss = 0.4310871362686157\ttest_accuracy = 0.6971401572227478\n",
            "step = 450\tloss = 0.41143521666526794\ttest_accuracy = 0.688701331615448\n",
            "step = 460\tloss = 0.42778322100639343\ttest_accuracy = 0.6821378469467163\n",
            "step = 470\tloss = 0.38684967160224915\ttest_accuracy = 0.6849507689476013\n",
            "step = 480\tloss = 0.40419355034828186\ttest_accuracy = 0.6952648758888245\n",
            "step = 490\tloss = 0.4028474986553192\ttest_accuracy = 0.7018284201622009\n",
            "step = 500\tloss = 0.4259778559207916\ttest_accuracy = 0.6821378469467163\n",
            "step = 510\tloss = 0.4004305303096771\ttest_accuracy = 0.6769807934761047\n",
            "step = 520\tloss = 0.4100584089756012\ttest_accuracy = 0.6952648758888245\n",
            "step = 530\tloss = 0.3987284302711487\ttest_accuracy = 0.703234851360321\n",
            "step = 540\tloss = 0.41889843344688416\ttest_accuracy = 0.7051101922988892\n",
            "step = 550\tloss = 0.4155591130256653\ttest_accuracy = 0.7022972106933594\n",
            "step = 560\tloss = 0.3977575898170471\ttest_accuracy = 0.7046413421630859\n",
            "step = 570\tloss = 0.4006323218345642\ttest_accuracy = 0.7065166234970093\n",
            "step = 580\tloss = 0.39414018392562866\ttest_accuracy = 0.6980777978897095\n",
            "step = 590\tloss = 0.3982029855251312\ttest_accuracy = 0.6971401572227478\n",
            "step = 600\tloss = 0.40112754702568054\ttest_accuracy = 0.700421929359436\n",
            "step = 610\tloss = 0.40060955286026\ttest_accuracy = 0.6840131282806396\n",
            "step = 620\tloss = 0.3955761790275574\ttest_accuracy = 0.6901078224182129\n",
            "step = 630\tloss = 0.39344701170921326\ttest_accuracy = 0.7018284201622009\n",
            "step = 640\tloss = 0.38515934348106384\ttest_accuracy = 0.7018284201622009\n",
            "step = 650\tloss = 0.3916786313056946\ttest_accuracy = 0.7008907794952393\n",
            "step = 660\tloss = 0.3846017122268677\ttest_accuracy = 0.6793248653411865\n",
            "step = 670\tloss = 0.39800846576690674\ttest_accuracy = 0.7102672457695007\n",
            "step = 680\tloss = 0.3977467119693756\ttest_accuracy = 0.7093296051025391\n",
            "step = 690\tloss = 0.3824683725833893\ttest_accuracy = 0.707454264163971\n",
            "step = 700\tloss = 0.38839593529701233\ttest_accuracy = 0.7079231142997742\n",
            "step = 710\tloss = 0.3916591703891754\ttest_accuracy = 0.6985466480255127\n",
            "step = 720\tloss = 0.3876110315322876\ttest_accuracy = 0.694796085357666\n",
            "step = 730\tloss = 0.37524738907814026\ttest_accuracy = 0.7037037014961243\n",
            "step = 740\tloss = 0.3918873965740204\ttest_accuracy = 0.7018284201622009\n",
            "step = 750\tloss = 0.3899511694908142\ttest_accuracy = 0.7041725516319275\n",
            "step = 760\tloss = 0.3805849850177765\ttest_accuracy = 0.6840131282806396\n",
            "step = 770\tloss = 0.373352974653244\ttest_accuracy = 0.6990154981613159\n",
            "step = 780\tloss = 0.38463160395622253\ttest_accuracy = 0.7051101922988892\n",
            "step = 790\tloss = 0.38647404313087463\ttest_accuracy = 0.7051101922988892\n",
            "step = 800\tloss = 0.39943256974220276\ttest_accuracy = 0.7051101922988892\n",
            "step = 810\tloss = 0.3805329501628876\ttest_accuracy = 0.7022972106933594\n",
            "step = 820\tloss = 0.39958277344703674\ttest_accuracy = 0.7008907794952393\n",
            "step = 830\tloss = 0.3877773582935333\ttest_accuracy = 0.6994842886924744\n",
            "step = 840\tloss = 0.3573869466781616\ttest_accuracy = 0.7055789828300476\n",
            "step = 850\tloss = 0.3787737786769867\ttest_accuracy = 0.703234851360321\n",
            "step = 860\tloss = 0.37821993231773376\ttest_accuracy = 0.694796085357666\n",
            "step = 870\tloss = 0.3829367458820343\ttest_accuracy = 0.7041725516319275\n",
            "step = 880\tloss = 0.3823745548725128\ttest_accuracy = 0.7088607549667358\n",
            "step = 890\tloss = 0.3814752399921417\ttest_accuracy = 0.6985466480255127\n",
            "step = 900\tloss = 0.38243401050567627\ttest_accuracy = 0.6966713666915894\n",
            "step = 910\tloss = 0.3774719834327698\ttest_accuracy = 0.7060478329658508\n",
            "step = 920\tloss = 0.3845384120941162\ttest_accuracy = 0.7065166234970093\n",
            "step = 930\tloss = 0.37696683406829834\ttest_accuracy = 0.7027660608291626\n",
            "step = 940\tloss = 0.38222184777259827\ttest_accuracy = 0.7027660608291626\n",
            "step = 950\tloss = 0.3724784851074219\ttest_accuracy = 0.6957337260246277\n",
            "step = 960\tloss = 0.36793604493141174\ttest_accuracy = 0.7041725516319275\n",
            "step = 970\tloss = 0.368844598531723\ttest_accuracy = 0.6985466480255127\n",
            "step = 980\tloss = 0.35775282979011536\ttest_accuracy = 0.7060478329658508\n",
            "step = 990\tloss = 0.37320730090141296\ttest_accuracy = 0.7093296051025391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-iwPbQl13WHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}