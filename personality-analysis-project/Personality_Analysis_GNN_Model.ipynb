{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Personality_Analysis_GNN_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyME1C6CcGuAGDXVtjeaLFgR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaspradhan/Graph-Neural-Networks/blob/main/personality-analysis-project/Personality_Analysis_GNN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U tf_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZIvNJlc0Q4q",
        "outputId": "7797e01f-a28f-4bad-d9e9-b424c8a8aa5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf_geometric in /usr/local/lib/python3.7/dist-packages (0.0.85)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17.4 in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (1.21.6)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (2.6.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (1.0.2)\n",
            "Requirement already satisfied: tf-sparse>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (0.0.12)\n",
            "Requirement already satisfied: ogb-lite>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from tf_geometric) (0.0.3)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb-lite>=0.0.3->tf_geometric) (0.2.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb-lite>=0.0.3->tf_geometric) (1.3.5)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb-lite>=0.0.3->tf_geometric) (1.24.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb-lite>=0.0.3->tf_geometric) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb-lite>=0.0.3->tf_geometric) (2.23.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb-lite>=0.0.3->tf_geometric) (0.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb-lite>=0.0.3->tf_geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb-lite>=0.0.3->tf_geometric) (2022.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->tf_geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->tf_geometric) (1.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb-lite>=0.0.3->tf_geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb-lite>=0.0.3->tf_geometric) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb-lite>=0.0.3->tf_geometric) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tf_geometric.utils import tf_utils\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "# # In case of any corpus are missing \n",
        "# download all-nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tf_geometric as tfg\n",
        "import pickle\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ0V2i680KFW",
        "outputId": "48af0bd8-e464-4fe4-ac1a-612b0f037736"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/decorators.py:70: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
            "  formatvalue=lambda value: \"\")[1:-1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ydm5E6rx5LDN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/mbti_cleaned.csv')\n"
      ],
      "metadata": {
        "id": "la-EiPTw0WNL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5TH1HXCW0f5l",
        "outputId": "b5eb212d-0e7b-42be-b7ee-cf2f91b9dac5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  type  Number of posts  \\\n",
              "0           0  INFJ               50   \n",
              "1           1  ENTP               50   \n",
              "2           2  INTP               50   \n",
              "3           3  INTJ               50   \n",
              "4           4  ENTJ               50   \n",
              "\n",
              "                                               Posts  \n",
              "0    intj moments    sportscenter    plays    pra...  \n",
              "1   finding  lack    these posts very alarmingsex...  \n",
              "2  good       course  which    know thats  blessi...  \n",
              "3  dear intp    enjoyed  conversation  other   es...  \n",
              "4  youre firedthats another silly misconception t...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a8573b02-ae49-4e1c-8a40-9d5fae0d3051\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>type</th>\n",
              "      <th>Number of posts</th>\n",
              "      <th>Posts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>INFJ</td>\n",
              "      <td>50</td>\n",
              "      <td>intj moments    sportscenter    plays    pra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ENTP</td>\n",
              "      <td>50</td>\n",
              "      <td>finding  lack    these posts very alarmingsex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>INTP</td>\n",
              "      <td>50</td>\n",
              "      <td>good       course  which    know thats  blessi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>INTJ</td>\n",
              "      <td>50</td>\n",
              "      <td>dear intp    enjoyed  conversation  other   es...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>ENTJ</td>\n",
              "      <td>50</td>\n",
              "      <td>youre firedthats another silly misconception t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8573b02-ae49-4e1c-8a40-9d5fae0d3051')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a8573b02-ae49-4e1c-8a40-9d5fae0d3051 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a8573b02-ae49-4e1c-8a40-9d5fae0d3051');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "posts = np.array(data['Posts'])\n",
        "posts.astype(np.dtype('str'))\n",
        "data['Posts']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQyZBoCT66EC",
        "outputId": "7f34863c-1a87-490c-8f99-07db299853e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         intj moments    sportscenter    plays    pra...\n",
              "1        finding  lack    these posts very alarmingsex...\n",
              "2       good       course  which    know thats  blessi...\n",
              "3       dear intp    enjoyed  conversation  other   es...\n",
              "4       youre firedthats another silly misconception t...\n",
              "                              ...                        \n",
              "7582     just because  always think  cats   doms  some...\n",
              "7583    soif this thread already exists someplace else...\n",
              "7584     many questions when   these things   would ta...\n",
              "7585      very conflicted right  when  comes  wanting ...\n",
              "7586      been  long since  have been  personalitycafe...\n",
              "Name: Posts, Length: 7587, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['type'].unique().shape[0] # 16 different personalities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CUp7x7l0hpQ",
        "outputId": "43224452-6a66-42db-b6bd-ad0b89d03009"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-ap7objCsnq",
        "outputId": "44cd5a6f-51d6-4a42-e825-85c74157e505"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0          0\n",
              "type                0\n",
              "Number of posts     0\n",
              "Posts              42\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "JV_A0KidCul0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 1 : 16 class classification"
      ],
      "metadata": {
        "id": "CBXGUIEn1BlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = data['Posts']"
      ],
      "metadata": {
        "id": "bM84fUjK5nym"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "labels = encoder.fit_transform(data['type'])"
      ],
      "metadata": {
        "id": "VoFIUc3J1ZSo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_-HMBGa5a4S",
        "outputId": "f628e41c-670a-4a6c-a4b0-29e9931b7f5e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = texts.astype('string')"
      ],
      "metadata": {
        "id": "zpJIHIS1B_lW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.3)"
      ],
      "metadata": {
        "id": "PRiqZVAH5hdi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_texts[:254])\n",
        "tokenizer.fit_on_texts(train_texts[256:])"
      ],
      "metadata": {
        "id": "xbQy2mvn5xYp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)"
      ],
      "metadata": {
        "id": "jom_Nh7k55Pk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PMIModel(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.word_counter = None\n",
        "        self.pair_counter = None\n",
        "\n",
        "    def get_pair_id(self, word0, word1):\n",
        "        pair_id = tuple(sorted([word0, word1]))\n",
        "        return pair_id\n",
        "\n",
        "    def fit(self, sequences, window_size):\n",
        "\n",
        "        self.word_counter = Counter()\n",
        "        self.pair_counter = Counter()\n",
        "        num_windows = 0\n",
        "        for sequence in tqdm(sequences):\n",
        "            for offset in range(len(sequence) - window_size):\n",
        "                window = sequence[offset:offset + window_size]\n",
        "                num_windows += 1\n",
        "                for i, word0 in enumerate(window):\n",
        "                    self.word_counter[word0] += 1\n",
        "                    for j, word1 in enumerate(window[i + 1:]):\n",
        "                        pair_id = self.get_pair_id(word0, word1)\n",
        "                        self.pair_counter[pair_id] += 1\n",
        "\n",
        "        for word, count in self.word_counter.items():\n",
        "            self.word_counter[word] = count / num_windows\n",
        "        for pair_id, count in self.pair_counter.items():\n",
        "            self.pair_counter[pair_id] = count / num_windows\n",
        "\n",
        "    def transform(self, word0, word1):\n",
        "        prob_a = self.word_counter[word0]\n",
        "        prob_b = self.word_counter[word1]\n",
        "        pair_id = self.get_pair_id(word0, word1)\n",
        "        prob_pair = self.pair_counter[pair_id]\n",
        "\n",
        "        if prob_a == 0 or prob_b == 0 or prob_pair == 0:\n",
        "            return 0\n",
        "\n",
        "        pmi = np.log(prob_pair / (prob_a * prob_b))\n",
        "        # print(word0, word1, pmi)\n",
        "        pmi = np.maximum(pmi, 0.0)\n",
        "        # print(pmi)\n",
        "        return pmi"
      ],
      "metadata": {
        "id": "IvuxFtzz6Rbg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_word_graph(num_words, pmi_model, embedding_size):\n",
        "    x = tf.Variable(tf.random.truncated_normal([num_words, embedding_size], stddev=1 / np.sqrt(embedding_size)),\n",
        "                    dtype=tf.float32)\n",
        "    edges = []\n",
        "    edge_weight = []\n",
        "    for (word0, word1) in pmi_model.pair_counter.keys():\n",
        "        pmi = pmi_model.transform(word0, word1)\n",
        "        if pmi > 0:\n",
        "            edges.append([word0, word1])\n",
        "            edge_weight.append(pmi)\n",
        "            edges.append([word1, word0])\n",
        "            edge_weight.append(pmi)\n",
        "    edge_index = np.array(edges).T\n",
        "    return tfg.Graph(x=x, edge_index=edge_index, edge_weight=edge_weight)"
      ],
      "metadata": {
        "id": "dLvrOi5B4QTb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_combined_graph(word_graph, sequences, embedding_size):\n",
        "    num_words = word_graph.num_nodes\n",
        "    x = tf.zeros([len(sequences), embedding_size], dtype=tf.float32)\n",
        "    edges = []\n",
        "    edge_weight = []\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        doc_node_index = num_words + i\n",
        "        for word in sequence:\n",
        "            edges.append([doc_node_index, word])  # only directed edge\n",
        "            edge_weight.append(1.0)  # use BOW instaead of TF-IDF\n",
        "\n",
        "    edge_index = np.array(edges).T\n",
        "    x = tf.concat([word_graph.x, x], axis=0)\n",
        "    edge_index = np.concatenate([word_graph.edge_index, edge_index], axis=1)\n",
        "    edge_weight = np.concatenate([word_graph.edge_weight, edge_weight], axis=0)\n",
        "    return tfg.Graph(x=x, edge_index=edge_index, edge_weight=edge_weight)"
      ],
      "metadata": {
        "id": "ss6Wx4yw4Stt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmi_cache_path = \"cached_pmi_model.p\"\n",
        "if os.path.exists(pmi_cache_path):\n",
        "    with open(pmi_cache_path, \"rb\") as f:\n",
        "        pmi_model = pickle.load(f)\n",
        "else:\n",
        "    pmi_model = PMIModel()\n",
        "    pmi_model.fit(train_sequences, window_size=6)\n",
        "    with open(pmi_cache_path, \"wb\") as f:\n",
        "        pickle.dump(pmi_model, f)\n",
        "\n",
        "embedding_size = 150\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "word_graph = build_word_graph(num_words, pmi_model, embedding_size)\n",
        "train_combined_graph = build_combined_graph(word_graph, train_sequences, embedding_size)\n",
        "test_combined_graph = build_combined_graph(word_graph, test_sequences, embedding_size)\n",
        "\n",
        "print(word_graph)\n",
        "print(train_combined_graph)\n",
        "print(test_combined_graph)\n",
        "\n",
        "num_classes = 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C1dplKZ4Vnj",
        "outputId": "1dc10749-b84c-45ea-a8b9-d01b5f0b564a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Shape: x => (165430, 150)\tedge_index => (2, 8508496)\ty => None\n",
            "Graph Shape: x => (170711, 150)\tedge_index => (2, 11226894)\ty => None\n",
            "Graph Shape: x => (167694, 150)\tedge_index => (2, 9630760)\ty => None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_combined_graph)\n",
        "print(test_combined_graph)"
      ],
      "metadata": {
        "id": "n7EdD-TdWhmp",
        "outputId": "b775ab6a-2540-49e4-a4b7-d87af409e6db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Shape: x => (167694, 150)\tedge_index => (2, 9630760)\ty => None\n",
            "Graph Shape: x => (167694, 150)\tedge_index => (2, 9630760)\ty => None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.gcn0 = tfg.layers.GCN(32, activation=tf.nn.relu)\n",
        "        self.gcn1 = tfg.layers.GCN(32,activation = tf.nn.relu)\n",
        "        self.gcn2 = tfg.layers.GCN(64,activation = tf.nn.relu)\n",
        "        self.gcn3 = tfg.layers.GCN(num_classes)\n",
        "        self.dropout = keras.layers.Dropout(0.5)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None, cache=None):\n",
        "        x, edge_index, edge_weight = inputs\n",
        "        h = self.gcn0([x, edge_index, edge_weight], cache=cache)\n",
        "        h = self.gcn1([h, edge_index, edge_weight],cache=cache)\n",
        "        h = self.dropout(h, training=training);\n",
        "        h = self.gcn2([h, edge_index, edge_weight],cache=cache)\n",
        "        h = self.gcn3([h, edge_index, edge_weight], cache=cache)\n",
        "        return h"
      ],
      "metadata": {
        "id": "lbFPPM6k4ZmB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCNModel()\n",
        "model.gcn0.cache_normed_edge(train_combined_graph)\n",
        "model.gcn0.cache_normed_edge(test_combined_graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYl7plHU-cQp",
        "outputId": "bb34d061-c79c-4d72-813f-63bf0d10edc4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tf_geometric/layers/conv/gcn.py:79: DeprecationWarning: 'GCN.cache_normed_edge(graph, override)' is deprecated, use 'GCN.build_cache_for_graph(graph, override)' instead\n",
            "  warnings.warn(\"'GCN.cache_normed_edge(graph, override)' is deprecated, use 'GCN.build_cache_for_graph(graph, override)' instead\", DeprecationWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/tf_geometric/layers/conv/gcn.py:79: DeprecationWarning: 'GCN.cache_normed_edge(graph, override)' is deprecated, use 'GCN.build_cache_for_graph(graph, override)' instead\n",
            "  warnings.warn(\"'GCN.cache_normed_edge(graph, override)' is deprecated, use 'GCN.build_cache_for_graph(graph, override)' instead\", DeprecationWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf_utils.function\n",
        "def forward(graph, training=False):\n",
        "    logits = model([graph.x, graph.edge_index, graph.edge_weight], cache=graph.cache, training=training)\n",
        "    logits = logits[num_words:]\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "tcIsXtoZ4dJA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(logits, labels):\n",
        "    losses = tf.nn.softmax_cross_entropy_with_logits(\n",
        "        logits=logits,\n",
        "        labels=tf.one_hot(labels, depth=num_classes)\n",
        "    )\n",
        "    # print(\"Transformed labels\", tf.one_hot(labels, depth=num_classes)[0])\n",
        "    mean_loss = tf.reduce_mean(losses)\n",
        "    return mean_loss"
      ],
      "metadata": {
        "id": "WPNkzHyb4eop"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)"
      ],
      "metadata": {
        "id": "YJ3BYYJp4gA0"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(1000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = forward(train_combined_graph, training=True)\n",
        "        # print(\"logits\" ,logits[0],\"Shape\",logits[0].shape)\n",
        "        mean_loss = compute_loss(logits, train_labels)\n",
        "\n",
        "    vars = tape.watched_variables()\n",
        "    grads = tape.gradient(mean_loss, vars)\n",
        "    optimizer.apply_gradients(zip(grads, vars))\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        # train accuracytf.one_hot(labels, depth=num_classes)\n",
        "        preds = tf.argmax(logits, axis=-1)\n",
        "        corrects = tf.cast(tf.equal(preds, train_labels), tf.float32)\n",
        "        train_accuracy = tf.reduce_mean(corrects)\n",
        "\n",
        "        logits = forward(test_combined_graph)\n",
        "        preds = tf.argmax(logits, axis=-1)\n",
        "        corrects = tf.cast(tf.equal(preds, test_labels), tf.float32)\n",
        "        accuracy = tf.reduce_mean(corrects)\n",
        "        print(\"step = {}\\tloss = {}\\ttrain_accuracy = {}\\ttest_accuracy = {}\".format(step, mean_loss, train_accuracy,accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbvtHG9M4iYo",
        "outputId": "88b6b057-99c0-4baf-bd2e-28b8620b812b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 0\tloss = 1.7703341245651245\ttrain_accuracy = 0.4145048260688782\ttest_accuracy = 0.20406360924243927\n",
            "step = 10\tloss = 1.7787028551101685\ttrain_accuracy = 0.4054156541824341\ttest_accuracy = 0.1992049515247345\n",
            "step = 20\tloss = 1.7721985578536987\ttrain_accuracy = 0.4139367640018463\ttest_accuracy = 0.208480566740036\n",
            "step = 30\tloss = 1.7809900045394897\ttrain_accuracy = 0.41128572821617126\ttest_accuracy = 0.19964663684368134\n",
            "step = 40\tloss = 1.7856276035308838\ttrain_accuracy = 0.4124218821525574\ttest_accuracy = 0.2031802088022232\n",
            "step = 50\tloss = 1.7928729057312012\ttrain_accuracy = 0.40825602412223816\ttest_accuracy = 0.2005300372838974\n",
            "step = 60\tloss = 1.769852876663208\ttrain_accuracy = 0.4199962019920349\ttest_accuracy = 0.20008833706378937\n",
            "step = 70\tloss = 1.8132067918777466\ttrain_accuracy = 0.4093921482563019\ttest_accuracy = 0.2018551230430603\n",
            "step = 80\tloss = 1.7808493375778198\ttrain_accuracy = 0.41620904207229614\ttest_accuracy = 0.19832155108451843\n",
            "step = 90\tloss = 1.7885150909423828\ttrain_accuracy = 0.4023859202861786\ttest_accuracy = 0.19832155108451843\n",
            "step = 100\tloss = 1.778863787651062\ttrain_accuracy = 0.40295398235321045\ttest_accuracy = 0.2045052945613861\n",
            "step = 110\tloss = 1.7582015991210938\ttrain_accuracy = 0.42567694187164307\ttest_accuracy = 0.20759716629981995\n",
            "step = 120\tloss = 1.7579959630966187\ttrain_accuracy = 0.4181026220321655\ttest_accuracy = 0.19876325130462646\n",
            "step = 130\tloss = 1.750814437866211\ttrain_accuracy = 0.4279492497444153\ttest_accuracy = 0.20494699478149414\n",
            "step = 140\tloss = 1.741376280784607\ttrain_accuracy = 0.4247301518917084\ttest_accuracy = 0.20362190902233124\n",
            "step = 150\tloss = 1.7497044801712036\ttrain_accuracy = 0.41639840602874756\ttest_accuracy = 0.20229682326316833\n",
            "step = 160\tloss = 1.7580459117889404\ttrain_accuracy = 0.4179132878780365\ttest_accuracy = 0.1978798657655716\n",
            "step = 170\tloss = 1.750124454498291\ttrain_accuracy = 0.4283279776573181\ttest_accuracy = 0.20097173750400543\n",
            "step = 180\tloss = 1.7416480779647827\ttrain_accuracy = 0.43249383568763733\ttest_accuracy = 0.20803886651992798\n",
            "step = 190\tloss = 1.7680963277816772\ttrain_accuracy = 0.41961750388145447\ttest_accuracy = 0.20273850858211517\n",
            "step = 200\tloss = 1.7866237163543701\ttrain_accuracy = 0.40598371624946594\ttest_accuracy = 0.20759716629981995\n",
            "step = 210\tloss = 1.7707080841064453\ttrain_accuracy = 0.41431546211242676\ttest_accuracy = 0.20671378076076508\n",
            "step = 220\tloss = 1.771650791168213\ttrain_accuracy = 0.4129899740219116\ttest_accuracy = 0.20406360924243927\n",
            "step = 230\tloss = 1.7486101388931274\ttrain_accuracy = 0.4281386137008667\ttest_accuracy = 0.20273850858211517\n",
            "step = 240\tloss = 1.7456802129745483\ttrain_accuracy = 0.433251291513443\ttest_accuracy = 0.2045052945613861\n",
            "step = 250\tloss = 1.7453224658966064\ttrain_accuracy = 0.43173640966415405\ttest_accuracy = 0.2071554809808731\n",
            "step = 260\tloss = 1.7466989755630493\ttrain_accuracy = 0.4209429919719696\ttest_accuracy = 0.20362190902233124\n",
            "step = 270\tloss = 1.7473948001861572\ttrain_accuracy = 0.4353342056274414\ttest_accuracy = 0.20141342282295227\n",
            "step = 280\tloss = 1.7480354309082031\ttrain_accuracy = 0.42132171988487244\ttest_accuracy = 0.2031802088022232\n",
            "step = 290\tloss = 1.7368433475494385\ttrain_accuracy = 0.4249195158481598\ttest_accuracy = 0.2045052945613861\n",
            "step = 300\tloss = 1.7516121864318848\ttrain_accuracy = 0.4311683475971222\ttest_accuracy = 0.2031802088022232\n",
            "step = 310\tloss = 1.7366487979888916\ttrain_accuracy = 0.4313577115535736\ttest_accuracy = 0.20494699478149414\n",
            "step = 320\tloss = 1.7182276248931885\ttrain_accuracy = 0.4393107295036316\ttest_accuracy = 0.20759716629981995\n",
            "step = 330\tloss = 1.7182769775390625\ttrain_accuracy = 0.43381935358047485\ttest_accuracy = 0.20627208054065704\n",
            "step = 340\tloss = 1.722758173942566\ttrain_accuracy = 0.42662373185157776\ttest_accuracy = 0.2098056524991989\n",
            "step = 350\tloss = 1.7354592084884644\ttrain_accuracy = 0.42965346574783325\ttest_accuracy = 0.2071554809808731\n",
            "step = 360\tloss = 1.725620150566101\ttrain_accuracy = 0.43173640966415405\ttest_accuracy = 0.20936395227909088\n",
            "step = 370\tloss = 1.7154462337493896\ttrain_accuracy = 0.4364703595638275\ttest_accuracy = 0.2071554809808731\n",
            "step = 380\tloss = 1.7063730955123901\ttrain_accuracy = 0.4404468834400177\ttest_accuracy = 0.20627208054065704\n",
            "step = 390\tloss = 1.7015461921691895\ttrain_accuracy = 0.4396894574165344\ttest_accuracy = 0.20627208054065704\n",
            "step = 400\tloss = 1.7172547578811646\ttrain_accuracy = 0.4353342056274414\ttest_accuracy = 0.20538869500160217\n",
            "step = 410\tloss = 1.7088534832000732\ttrain_accuracy = 0.4417724013328552\ttest_accuracy = 0.2018551230430603\n",
            "step = 420\tloss = 1.6942873001098633\ttrain_accuracy = 0.4444234073162079\ttest_accuracy = 0.208480566740036\n",
            "step = 430\tloss = 1.70392906665802\ttrain_accuracy = 0.4406362473964691\ttest_accuracy = 0.20671378076076508\n",
            "step = 440\tloss = 1.7248438596725464\ttrain_accuracy = 0.43779587745666504\ttest_accuracy = 0.2005300372838974\n",
            "step = 450\tloss = 1.7093305587768555\ttrain_accuracy = 0.4425298273563385\ttest_accuracy = 0.20406360924243927\n",
            "step = 460\tloss = 1.6951148509979248\ttrain_accuracy = 0.44082561135292053\ttest_accuracy = 0.20671378076076508\n",
            "step = 470\tloss = 1.7000259160995483\ttrain_accuracy = 0.4448021352291107\ttest_accuracy = 0.20803886651992798\n",
            "step = 480\tloss = 1.7094827890396118\ttrain_accuracy = 0.4465063512325287\ttest_accuracy = 0.2045052945613861\n",
            "step = 490\tloss = 1.6938074827194214\ttrain_accuracy = 0.4457489252090454\ttest_accuracy = 0.21201413869857788\n",
            "step = 500\tloss = 1.7133207321166992\ttrain_accuracy = 0.4290854036808014\ttest_accuracy = 0.19699646532535553\n",
            "step = 510\tloss = 1.7137157917022705\ttrain_accuracy = 0.43779587745666504\ttest_accuracy = 0.21201413869857788\n",
            "step = 520\tloss = 1.7001127004623413\ttrain_accuracy = 0.4444234073162079\ttest_accuracy = 0.20273850858211517\n",
            "step = 530\tloss = 1.6991547346115112\ttrain_accuracy = 0.439500093460083\ttest_accuracy = 0.20538869500160217\n",
            "step = 540\tloss = 1.7023258209228516\ttrain_accuracy = 0.4423404633998871\ttest_accuracy = 0.2005300372838974\n",
            "step = 550\tloss = 1.72030770778656\ttrain_accuracy = 0.4381745755672455\ttest_accuracy = 0.20538869500160217\n",
            "step = 560\tloss = 1.732961893081665\ttrain_accuracy = 0.43665972352027893\ttest_accuracy = 0.2045052945613861\n",
            "step = 570\tloss = 1.7053239345550537\ttrain_accuracy = 0.4423404633998871\ttest_accuracy = 0.2058303952217102\n",
            "step = 580\tloss = 1.6864943504333496\ttrain_accuracy = 0.4444234073162079\ttest_accuracy = 0.20273850858211517\n",
            "step = 590\tloss = 1.7075679302215576\ttrain_accuracy = 0.44423404335975647\ttest_accuracy = 0.20229682326316833\n",
            "step = 600\tloss = 1.68296480178833\ttrain_accuracy = 0.4525657892227173\ttest_accuracy = 0.20538869500160217\n",
            "step = 610\tloss = 1.7240368127822876\ttrain_accuracy = 0.439500093460083\ttest_accuracy = 0.20362190902233124\n",
            "step = 620\tloss = 1.7100369930267334\ttrain_accuracy = 0.42643439769744873\ttest_accuracy = 0.20671378076076508\n",
            "step = 630\tloss = 1.7080869674682617\ttrain_accuracy = 0.441204309463501\ttest_accuracy = 0.2045052945613861\n",
            "step = 640\tloss = 1.7069896459579468\ttrain_accuracy = 0.4406362473964691\ttest_accuracy = 0.20406360924243927\n",
            "step = 650\tloss = 1.6966543197631836\ttrain_accuracy = 0.4427191913127899\ttest_accuracy = 0.2018551230430603\n",
            "step = 660\tloss = 1.6966361999511719\ttrain_accuracy = 0.44309788942337036\ttest_accuracy = 0.20229682326316833\n",
            "step = 670\tloss = 1.6795933246612549\ttrain_accuracy = 0.4497254192829132\ttest_accuracy = 0.20494699478149414\n",
            "step = 680\tloss = 1.7041921615600586\ttrain_accuracy = 0.4434766173362732\ttest_accuracy = 0.20362190902233124\n",
            "step = 690\tloss = 1.7161788940429688\ttrain_accuracy = 0.4292747676372528\ttest_accuracy = 0.2045052945613861\n",
            "step = 700\tloss = 1.7270066738128662\ttrain_accuracy = 0.42870667576789856\ttest_accuracy = 0.2058303952217102\n",
            "step = 710\tloss = 1.6997021436691284\ttrain_accuracy = 0.445559561252594\ttest_accuracy = 0.20406360924243927\n",
            "step = 720\tloss = 1.8148740530014038\ttrain_accuracy = 0.4304109215736389\ttest_accuracy = 0.20362190902233124\n",
            "step = 730\tloss = 1.760102391242981\ttrain_accuracy = 0.42132171988487244\ttest_accuracy = 0.20759716629981995\n",
            "step = 740\tloss = 1.7171099185943604\ttrain_accuracy = 0.4323045015335083\ttest_accuracy = 0.20671378076076508\n",
            "step = 750\tloss = 1.7200204133987427\ttrain_accuracy = 0.439500093460083\ttest_accuracy = 0.2045052945613861\n",
            "step = 760\tloss = 1.69983971118927\ttrain_accuracy = 0.4453701972961426\ttest_accuracy = 0.2031802088022232\n",
            "step = 770\tloss = 1.7048373222351074\ttrain_accuracy = 0.4406362473964691\ttest_accuracy = 0.20759716629981995\n",
            "step = 780\tloss = 1.7003979682922363\ttrain_accuracy = 0.44593825936317444\ttest_accuracy = 0.20229682326316833\n",
            "step = 790\tloss = 1.6842857599258423\ttrain_accuracy = 0.44688504934310913\ttest_accuracy = 0.20494699478149414\n",
            "step = 800\tloss = 1.6884161233901978\ttrain_accuracy = 0.45124030113220215\ttest_accuracy = 0.20229682326316833\n",
            "step = 810\tloss = 1.6762971878051758\ttrain_accuracy = 0.44802120327949524\ttest_accuracy = 0.2031802088022232\n",
            "step = 820\tloss = 1.6776336431503296\ttrain_accuracy = 0.45105093717575073\ttest_accuracy = 0.20097173750400543\n",
            "step = 830\tloss = 1.6801705360412598\ttrain_accuracy = 0.44726377725601196\ttest_accuracy = 0.2018551230430603\n",
            "step = 840\tloss = 1.6639196872711182\ttrain_accuracy = 0.45010414719581604\ttest_accuracy = 0.2045052945613861\n",
            "step = 850\tloss = 1.67732572555542\ttrain_accuracy = 0.44707441329956055\ttest_accuracy = 0.20273850858211517\n",
            "step = 860\tloss = 1.6913082599639893\ttrain_accuracy = 0.4485892951488495\ttest_accuracy = 0.20538869500160217\n",
            "step = 870\tloss = 1.6896835565567017\ttrain_accuracy = 0.4434766173362732\ttest_accuracy = 0.2018551230430603\n",
            "step = 880\tloss = 1.683159351348877\ttrain_accuracy = 0.44688504934310913\ttest_accuracy = 0.20141342282295227\n",
            "step = 890\tloss = 1.6627414226531982\ttrain_accuracy = 0.4610869288444519\ttest_accuracy = 0.20273850858211517\n",
            "step = 900\tloss = 1.67368745803833\ttrain_accuracy = 0.4525657892227173\ttest_accuracy = 0.20141342282295227\n",
            "step = 910\tloss = 1.6819815635681152\ttrain_accuracy = 0.451808363199234\ttest_accuracy = 0.20008833706378937\n",
            "step = 920\tloss = 1.681422233581543\ttrain_accuracy = 0.45048287510871887\ttest_accuracy = 0.2005300372838974\n",
            "step = 930\tloss = 1.6643354892730713\ttrain_accuracy = 0.4557848870754242\ttest_accuracy = 0.20008833706378937\n",
            "step = 940\tloss = 1.65642511844635\ttrain_accuracy = 0.45862525701522827\ttest_accuracy = 0.19611307978630066\n",
            "step = 950\tloss = 1.6574307680130005\ttrain_accuracy = 0.4608975648880005\ttest_accuracy = 0.20008833706378937\n",
            "step = 960\tloss = 1.6685621738433838\ttrain_accuracy = 0.44802120327949524\ttest_accuracy = 0.20097173750400543\n",
            "step = 970\tloss = 1.689611792564392\ttrain_accuracy = 0.4453701972961426\ttest_accuracy = 0.1992049515247345\n",
            "step = 980\tloss = 1.6843130588531494\ttrain_accuracy = 0.45218709111213684\ttest_accuracy = 0.19832155108451843\n",
            "step = 990\tloss = 1.6605353355407715\ttrain_accuracy = 0.4557848870754242\ttest_accuracy = 0.19964663684368134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_pGE7H0CRctA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 2 - 4 binary classifiers - one hot encoding"
      ],
      "metadata": {
        "id": "z4VCI2M2U34l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Oz4QJniIU7Tw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}